from typing import Any

from agentkit.services.llm import create_llm
from langchain_core.prompts import ChatPromptTemplate
from langsmith.evaluation import EvaluationResult, RunEvaluator
from langsmith.schemas import Example, Run

from foundermode.config import settings


class InvestorRubricEvaluator(RunEvaluator):  # type: ignore
    """
    Evaluates an Investment Memo against a professional Investor Rubric.
    """

    def __init__(self, model_name: str | None = None):
        self.model_name = model_name or settings.model_name
        self.eval_llm = create_llm(model=self.model_name)

        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are a General Partner at a top-tier Venture Capital firm (e.g., Sequoia, Benchmark).
            You are grading an Investment Memo generated by an AI Associate.

            Rubric:
            1. **Hallucination Check:** Does the memo make wild claims without citation? (Pass/Fail)
            2. **Analytical Depth:** Does it discuss \"Why now?\", \"Moats\", and \"Unit Economics\"? (1-5 Score)
            3. **Relevance:** Is the content specific to the industry, or generic fluff? (1-5 Score)

            Output your evaluation in the following format:
            Score: <Overall Score 1-10>
            Reasoning: <Concise explanation>
            """,
                ),
                (
                    "human",
                    """
            Research Question: {input}

            Generated Memo:
            {output}
            """,
                ),
            ]
        )

    def evaluate_run(  # type: ignore[override]
        self, run: Run, example: Example | None = None, **kwargs: Any
    ) -> EvaluationResult:
        if not run.outputs or "memo_draft" not in run.outputs:
            return EvaluationResult(key="investor_score", score=0, comment="No memo generated")

        # Extract the memo content
        # Depending on how the graph output is structured, we might need to parse pydantic object
        memo = run.outputs["memo_draft"]
        memo_text = ""
        if hasattr(memo, "dict"):
            memo_dict = memo.dict()
            memo_text = (
                f"Exec Summary: {memo_dict.get('executive_summary')}\n"
                f"Market: {memo_dict.get('market_analysis')}\n"
                f"Competition: {memo_dict.get('competitive_landscape')}"
            )
        else:
            memo_text = str(memo)

        input_q = run.inputs.get("research_question", "Unknown") if run.inputs else "Unknown"

        # Run Evaluation
        chain = self.prompt | self.eval_llm
        response = chain.invoke({"input": input_q, "output": memo_text})

        # Parse the score (Naive parsing for now)
        content = response.content
        try:
            # Look for "Score: X"
            import re

            match = re.search(r"Score:\s*(\d+)", str(content))
            score = int(match.group(1)) if match else 0
        except Exception:
            score = 0

        return EvaluationResult(
            key="investor_score",
            score=score / 10.0,  # Normalize to 0-1
            comment=str(content),
        )


class PlannerEvaluator(RunEvaluator):  # type: ignore
    """
    Evaluates the quality of search queries generated by the Planner node.
    Checks for "Due Diligence Intent" (Unit Economics, TAM, Moats).
    """

    def __init__(self, model_name: str | None = None):
        self.model_name = model_name or settings.model_name
        self.eval_llm = create_llm(model=self.model_name)

        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are an Expert Due Diligence Analyst.
            Review the following search queries generated by an AI research agent.

            Evaluate if the queries show high "Due Diligence Intent".
            High intent queries target:
            - Unit Economics (CAC, LTV, Gross Margins)
            - Market Sizing (TAM, SAM, SOM with specific years/segments)
            - Competitive Moats (Network effects, switching costs, IP)
            - Incumbent weaknesses or customer pain points.

            Low intent queries are generic or fluffy (e.g., "AI trends", "startup ideas").

            Output your evaluation:
            Score: <0-1.0>
            Reasoning: <Explanation of the score>
            """,
                ),
                (
                    "human",
                    "Research Question: {input}\n\nSearch Queries:\n{queries}",
                ),
            ]
        )

    def evaluate_run(  # type: ignore[override]
        self, run: Run, example: Example | None = None, **kwargs: Any
    ) -> EvaluationResult:
        # Extract all 'research_topic' updates from the planner node in the trace
        # This requires traversing child runs if LangSmith didn't flatten the output
        queries = []

        def _collect_queries(r: Run) -> None:
            if r.name == "planner" and r.outputs and "research_topic" in r.outputs:
                topic = r.outputs["research_topic"]
                if topic:
                    queries.append(topic)
            if r.child_runs:
                for child in r.child_runs:
                    _collect_queries(child)

        _collect_queries(run)

        if not queries:
            return EvaluationResult(key="planner_intent_score", score=0, comment="No search queries found")

        input_q = run.inputs.get("research_question", "Unknown") if run.inputs else "Unknown"

        # Run Evaluation
        chain = self.prompt | self.eval_llm
        response = chain.invoke({"input": input_q, "queries": "\n".join(queries)})

        # Parse the score
        content = str(response.content)
        try:
            import re

            match = re.search(r"Score:\s*([\d.]+)", content)
            score = float(match.group(1)) if match else 0.0
        except Exception:
            score = 0.0

        return EvaluationResult(
            key="planner_intent_score",
            score=min(max(score, 0.0), 1.0),
            comment=content,
        )


class ResearcherEvaluator(RunEvaluator):  # type: ignore
    """
    Measures the Signal-to-Noise ratio of the Researcher node.
    Score = (Number of unique facts) / (Number of URLs attempted)
    """

    def evaluate_run(  # type: ignore[override]
        self, run: Run, example: Example | None = None, **kwargs: Any
    ) -> EvaluationResult:
        all_facts = []

        def _collect_metrics(r: Run) -> None:
            if r.name == "researcher" and r.outputs:
                # Assuming research_facts is a list of objects with 'content'
                facts = r.outputs.get("research_facts", [])
                all_facts.extend([f.get("content") for f in facts if isinstance(f, dict) and f.get("content")])

                # This is a bit tricky since urls_attempted isn't explicitly in output
                # We can count unique sources in facts or logs if available.
                # For now, let's use the number of unique sources in the facts as a proxy for 'successful' URLs.
                # Better: check if we can find 'urls_attempted' in logs/metadata.
            if r.child_runs:
                for child in r.child_runs:
                    _collect_metrics(child)

        _collect_metrics(run)

        # Unique facts
        unique_facts = len(set(all_facts))

        # Proxy for noise: total calls to researcher.
        # Let's try to count how many times researcher was called.
        research_calls = 0

        def _count_calls(r: Run) -> None:
            nonlocal research_calls
            if r.name == "researcher":
                research_calls += 1
            if r.child_runs:
                for child in r.child_runs:
                    _count_calls(child)

        _count_calls(run)

        if research_calls == 0:
            return EvaluationResult(key="researcher_efficiency", score=0, comment="No research performed")

        # Score = Average facts per call, capped at 1.0 for the metric normalized range?
        # Actually SNR as defined in spec is Facts / URLs.
        # If we don't have URL count, let's use facts/calls.
        score = unique_facts / (research_calls * 5)  # Normalize assuming 5 facts per call is 'good'

        return EvaluationResult(
            key="researcher_efficiency",
            score=min(max(score, 0.0), 1.0),
            comment=f"Facts: {unique_facts}, Research Calls: {research_calls}",
        )


class RevisionDeltaEvaluator(RunEvaluator):  # type: ignore
    """
    Quantifies the impact of the Critic node.
    Compares the first and last memo drafts to see if analytical depth increased.
    """

    def __init__(self, model_name: str | None = None):
        self.model_name = model_name or settings.model_name
        self.eval_llm = create_llm(model=self.model_name)

        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are a Revision Auditor. Compare two versions of an investment memo.
            Version 1 is the initial draft.
            Version 2 is the draft after receiving adversarial feedback.

            Evaluate if Version 2 is significantly better.
            A better version 2 MUST:
            - Include more specific quantitative facts (numbers, percentages, dollar amounts).
            - Address specific risks that were missing in Version 1.
            - Show deeper competitive analysis.

            Output:
            Improvement_Score: <0.0 - 1.0> (0.0 = no improvement, 1.0 = massive improvement)
            Reasoning: <Explanation>
            """,
                ),
                (
                    "human",
                    "Version 1:\n{v1}\n\nVersion 2:\n{v2}",
                ),
            ]
        )

    def evaluate_run(  # type: ignore[override]
        self, run: Run, example: Example | None = None, **kwargs: Any
    ) -> EvaluationResult:
        drafts = []

        def _collect_drafts(r: Run) -> None:
            if r.name == "writer" and r.outputs and "memo_draft" in r.outputs:
                drafts.append(r.outputs["memo_draft"])
            if r.child_runs:
                for child in r.child_runs:
                    _collect_drafts(child)

        _collect_drafts(run)

        if len(drafts) < 2:
            return EvaluationResult(
                key="revision_delta", score=0, comment="No revisions occurred (only one draft produced)"
            )

        v1 = str(drafts[0])
        v2 = str(drafts[-1])

        # Run Evaluation
        chain = self.prompt | self.eval_llm
        response = chain.invoke({"v1": v1, "v2": v2})

        # Parse the score
        content = str(response.content)
        try:
            import re

            match = re.search(r"Improvement_Score:\s*([\d.]+)", content)
            score = float(match.group(1)) if match else 0.0
        except Exception:
            score = 0.0

        return EvaluationResult(
            key="revision_delta",
            score=min(max(score, 0.0), 1.0),
            comment=content,
        )


class HallucinationEvaluator(RunEvaluator):  # type: ignore
    """
    Checks the final Investment Memo for hallucinations by cross-referencing with collected facts.
    """

    def __init__(self, model_name: str | None = None):
        self.model_name = model_name or settings.model_name
        self.eval_llm = create_llm(model=self.model_name)

        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are a Fact Checker. Your job is to determine if the claims in the Investment Memo
            are supported by the Research Facts provided.

            Focus on quantitative claims (market sizes, prices, dates).
            If a specific numeric claim is NOT supported or is contradicted by the facts,
            mark it as a hallucination.

            Output:
            Hallucination_Found: <Yes/No>
            Score: <1.0 if no hallucinations, 0.0 if any found>
            Reasoning: <Explanation identifying specific unsupported claims>
            """,
                ),
                (
                    "human",
                    "Research Facts:\n{facts}\n\nGenerated Memo:\n{memo}",
                ),
            ]
        )

    def evaluate_run(  # type: ignore[override]
        self, run: Run, example: Example | None = None, **kwargs: Any
    ) -> EvaluationResult:
        if not run.outputs or "memo_draft" not in run.outputs:
            return EvaluationResult(key="hallucination_score", score=0, comment="No memo to check")

        # Collect all facts from the trace
        all_facts = []

        def _collect_facts(r: Run) -> None:
            if r.name == "researcher" and r.outputs and "research_facts" in r.outputs:
                facts = r.outputs["research_facts"]
                for f in facts:
                    if isinstance(f, dict):
                        all_facts.append(f.get("content", ""))
                    else:
                        all_facts.append(str(f))
            if r.child_runs:
                for child in r.child_runs:
                    _collect_facts(child)

        _collect_facts(run)
        facts_str = "\n".join(all_facts)

        memo = str(run.outputs["memo_draft"])

        # Run Evaluation
        chain = self.prompt | self.eval_llm
        response = chain.invoke({"facts": facts_str[:15000], "memo": memo})

        # Parse the score
        content = str(response.content)
        try:
            import re

            match = re.search(r"Score:\s*([\d.]+)", content)
            score = float(match.group(1)) if match else 1.0
        except Exception:
            score = 1.0

        return EvaluationResult(
            key="hallucination_score",
            score=min(max(score, 0.0), 1.0),
            comment=content,
        )
