from typing import Any

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langsmith.evaluation import EvaluationResult, RunEvaluator
from langsmith.schemas import Example, Run

from foundermode.config import settings


class InvestorRubricEvaluator(RunEvaluator):  # type: ignore
    """
    Evaluates an Investment Memo against a professional Investor Rubric.
    """

    def __init__(self, model_name: str = "gpt-4o"):
        self.model_name = model_name
        self.eval_llm = ChatOpenAI(model=model_name, temperature=0, openai_api_key=settings.openai_api_key)

        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are a General Partner at a top-tier Venture Capital firm (e.g., Sequoia, Benchmark).
            You are grading an Investment Memo generated by an AI Associate.

            Rubric:
            1. **Hallucination Check:** Does the memo make wild claims without citation? (Pass/Fail)
            2. **Analytical Depth:** Does it discuss \"Why now?\", \"Moats\", and \"Unit Economics\"? (1-5 Score)
            3. **Relevance:** Is the content specific to the industry, or generic fluff? (1-5 Score)

            Output your evaluation in the following format:
            Score: <Overall Score 1-10>
            Reasoning: <Concise explanation>
            """,
                ),
                (
                    "human",
                    """
            Research Question: {input}

            Generated Memo:
            {output}
            """,
                ),
            ]
        )

    def evaluate_run(self, run: Run, example: Example | None = None, **kwargs: Any) -> EvaluationResult:
        if not run.outputs or "memo_draft" not in run.outputs:
            return EvaluationResult(key="investor_score", score=0, comment="No memo generated")

        # Extract the memo content
        # Depending on how the graph output is structured, we might need to parse pydantic object
        memo = run.outputs["memo_draft"]
        memo_text = ""
        if hasattr(memo, "dict"):
            memo_dict = memo.dict()
            memo_text = (
                f"Exec Summary: {memo_dict.get('executive_summary')}\n"
                f"Market: {memo_dict.get('market_analysis')}\n"
                f"Competition: {memo_dict.get('competitive_landscape')}"
            )
        else:
            memo_text = str(memo)

        input_q = run.inputs.get("research_question", "Unknown") if run.inputs else "Unknown"

        # Run Evaluation
        chain = self.prompt | self.eval_llm
        response = chain.invoke({"input": input_q, "output": memo_text})

        # Parse the score (Naive parsing for now)
        content = response.content
        try:
            # Look for "Score: X"
            import re

            match = re.search(r"Score:\s*(\d+)", str(content))
            score = int(match.group(1)) if match else 0
        except Exception:
            score = 0

        return EvaluationResult(
            key="investor_score",
            score=score / 10.0,  # Normalize to 0-1
            comment=str(content),
        )
